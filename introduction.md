% \section{Abstract}
% Advancements in computer vision and natural language processing have transformed healthcare by enhancing patient care, diagnostics, and treatments. This thesis develops tailored deep learning methods to address key challenges in applying machine learning to healthcare. I introduce a novel flexible knowledge distillation approach to compress and transfer information from large vision models to smaller, domain-specific models optimized for resource-constrained settings. Additionally, I designed an information-efficient contrastive vision-language alignment method designed to learn from limited annotated data using smaller batch sizes, achieving superior performance on standard benchmarks in resource-constrained settings. To minimize visual variations in medical images due to inconsistencies in data collection and processing, I develop structure-preserving generative adversarial networks for stain normalization, improving the robustness of downstream analyses. Furthermore, I design an end-to-end mechanism using a conditional diffusion model to generate synthetic hyper-realistic tissue patches, addressing data scarcity and privacy concerns while enhancing both machine learning training and human pathological analysis. By integrating these methodologies, the thesis offers innovative solutions to longstanding challenges in medical data analysis and diagnostics, highlighting the critical role of tailored machine learning techniques in improving patient outcomes and elevating healthcare standards.

In recent years, the field of multi-disciplinary artificial intelligence has seen tremendous progress driven by computer vision and natural language processing advancements. Recent work in these fields has enabled computers to better understand and interpret visual information from images, and accurately comprehend and generate human language. Extending these capabilities to data- and resource-constrained settings presents unique challenges but can have transformative change. These innovations have also significantly transformed the healthcare landscape by offering unprecedented opportunities to enhance patient care, improve diagnostic accuracy, and optimize treatment strategies. In this thesis, I present my contributions to tackle major challenges in tailored application of state-of-the-art machine learning techniques to low data and resource settings. 

One key challenge is adapting general-use, real-world models into smaller, domain-specific models. This is achieved via knowledge distillation. Foundation models are large-scale artificial intelligence models that have been trained on vast amounts of general data. They possess a broad understanding of language and images but are not tailored to specific tasks. Distilling information from these models means extracting and refining the valuable knowledge they contain to create smaller, more specialized models suited for specific applications. In my work, we address this challenge by introducing a flexible knowledge distillation method that can be optimized in severe resource-constrained settings and can work with a varied range of model architectures.

Another challenge in multi-disciplinary application of computer vision is a lack to annotations. However, textual annotations are relatively easy to obtain and are often abundant. Leveraging these readily available text annotations reduces the need for extensive manual labeling of images, which is both time-consuming and costly. By using this existing textual data, vision models can efficiently learn to associate visual features with semantic concepts, improving their performance and generalization. This process requires what's known as vision-language alignment, where the model learns to associate visual features from images with relevant textual descriptions using large paired image-text datasets. In this work, we design alignment models to learn effectively from small annotated datasets. We present a vision-language alignment objective that is designed to be trained with significantly reduced amounts of data and using smaller batch sizes while providing comparable or superior performance on standard benchmarks against other methods trained in similar settings.

In medical image analysis, machine learning models often suffer from bias and reduced performance due to variations in data from different sample sources. In histopathology, these differences manifest as stain variations which are discernible variations in color of the slides that arise from inconsistencies in laboratory procedures, differences in staining reagents, and variations in imaging equipment. These variations can negatively impact the performance of automated image analysis algorithms, such as those used for nuclei detection, tissue segmentation, or disease classification. Such inconsistencies can impair the ability of models to generalize to new, differently stained images. We tackle this problem by introducing a novel adversarial method that executes many-to-one domain stain normalization. The training objective is designed to make sure that the structure of the image is preserved during translation. Our method demonstrates impressive performance in preserving the structural integrity of images while transferring the stain distributions when tested on duodenum biopsies.

Model training and diagnostics for medical imaging applications suffers from pervasive lack of data due to ethical and privacy concerns. Specifically in the field of pathology, histopathological analysis relies on hematoxylin and eosin (H\&E) stained biopsies for microscopic inspection to identify diseases, including cancers, with diagnosis heavily dependent on the pathologist's training and exposure to various disease subtypes. This presents challenges, especially with rare variants, which are harder to identify visually. Recently, deep learning methods have been developed to support diagnosis, particularly through segmentation models that identify nuclei types. Generative models can generate histopathology images with specific characteristics, addressing the imbalance in datasets and reducing bias in model training. These models hold potential to improve diagnosis by aiding both deep learning systems and human pathologists, and synthetic datasets can help overcome privacy concerns in medical data sharing. Conditional generation of annotated data adds value by alleviating the high costs of labeling medical images. This synthetic data can also be used to train machine learning models without compromising patient privacy, thereby overcoming the limitations posed by scarce or sensitive data.

Overall, this thesis makes contributions towards extending state-of-the-art machine learning techniques to low data and resource settings on multiple fronts. Our proposed knowledge distillation techniques facilitate seamless knowledge transfer between neural networks, enabling effective model compression and transfer learning from foundational vision and language models. We present methodology for leveraging textual captions for vision model pretraining in resource and data-constrained environments. We also present a stain-normalization methodology specifically addressing the problem of variation in visual appearance of digital slides due to differences in staining mechanisms across sites. Further, our examination of conditional diffusion models for generative modeling of histopathological tissue slides holds substantial promise for the field of pathology. By synthesizing tissue patches conditioned on nuclei masks, this approach presents a pioneering solution for enhancing the accuracy and efficiency of histopathological analysis by addressing the pervasive lack of annotated data in medical imaging analysis. As such, my thesis statement is:

Tailored deep learning methods enable consistent and critical progress toward enhancing reasoning capabilities, particularly in data- and resource-constrained settings like healthcare. Improvements in knowledge distillation using information maximization enables cheaper optimization and domain-transfer for large vision models. Information-efficient contrastive learning for aligning images with textual data leads to better performance on small datasets. Structure-preserving generative adversarial networks help minimize visual variations in medical images. Conditional diffusion can be used to develop end-to-end models for synthesizing inherently-annotated histology tissue samples with pixel-perfect nuclei localization.

This thesis makes the following novel contributions:

1. **Knowledge distillation**
   - We developed three flexible mutual information maximization objectives for knowledge distillation.
   - Our method is effective across a wide range of model pairs and enables learning transferable representations.

2. **Contrastive image & text alignment**
   - Our work allows training multi-modal alignment models in data and resource-constrained settings.
   - The method achieves state-of-the-art performance in downstream tasks like retrieval, unsupervised object localization, and zero-shot learning.

3. **Stain normalization for H&E images**
   - We designed a structure-preserving cycle-consistent architecture for unpaired image-to-image translation to normalize color distributions.
   - The method achieves unprecedented performance in normalizing stain distributions in histology images.

4. **Generative modeling for medical imaging**
   - Our end-to-end method synthesizes unlimited annotated realistic histology tissue samples with pixel-perfect nuclei localization.
   - It demonstrates competitive metrics quantitatively, and our expert qualitative evaluations suggest that synthetic patches are comparable to the real set.
